<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conventional ML Deep Dive</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        :root {
            color-scheme: light;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #f7f9fc;
            margin: 0;
            padding: 40px 20px 60px;
            color: #19202c;
        }

        .container {
            max-width: 1080px;
            margin: 0 auto;
        }

        header h1 {
            font-size: 2.4rem;
            margin-bottom: 8px;
        }

        header p {
            color: #475569;
            margin-bottom: 32px;
            line-height: 1.6;
        }

        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 28px;
        }

        section.card {
            background: white;
            border-radius: 18px;
            padding: 26px;
            box-shadow: 0 22px 48px rgba(15, 23, 42, 0.08);
            border: 1px solid rgba(62, 74, 177, 0.12);
        }

        h2, h3 {
            color: #1f2937;
            margin-top: 0;
        }

        h2 {
            font-size: 1.45rem;
            margin-bottom: 18px;
        }

        h3 {
            font-size: 1.1rem;
            margin-top: 22px;
            margin-bottom: 12px;
        }

        ul, ol {
            padding-left: 20px;
            color: #404b5a;
            line-height: 1.6;
        }

        .badge {
            display: inline-flex;
            align-items: center;
            padding: 4px 12px;
            border-radius: 999px;
            background: rgba(79, 70, 229, 0.12);
            color: #4338ca;
            font-size: 0.75rem;
            font-weight: 600;
            margin-bottom: 16px;
        }

        .split {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 22px;
        }

        .diagram {
            background: #eef2ff;
            border: 1px dashed rgba(79, 70, 229, 0.4);
            border-radius: 16px;
            padding: 18px;
            font-size: 0.95rem;
            color: #312e81;
            line-height: 1.6;
        }

        code {
            font-family: 'Fira Code', 'Cascadia Code', 'Consolas', monospace;
            background: rgba(15, 23, 42, 0.08);
            border-radius: 6px;
            padding: 2px 6px;
            font-size: 0.85rem;
        }

        .qa {
            margin-top: 18px;
            padding: 18px;
            background: #f8fafc;
            border: 1px solid rgba(15, 23, 42, 0.08);
            border-radius: 12px;
        }

        .qa strong {
            display: block;
            margin-bottom: 6px;
            color: #0f172a;
        }

        .nav-link {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            margin-top: 32px;
            font-weight: 600;
            color: #4338ca;
            text-decoration: none;
        }

        @media (max-width: 720px) {
            body {
                padding: 28px 16px 48px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="badge">Conventional ML System Design</div>
            <h1>Interview-Ready Conventional ML Concepts</h1>
            <p>Use this playbook to answer architecture-style questions about supervised & unsupervised learning systems. Each block highlights the decisions interviewers press on: data flows, algorithm trade-offs, tuning levers, and production hardening.</p>
        </header>

        <div class="grid">
            <section class="card">
                <h2>Problem Framing Cheat Sheet</h2>
                <ul>
                    <li><strong>Signal vs. noise:</strong> Define target, business KPI, decision latency, and tolerable error range.</li>
                    <li><strong>Data audit:</strong> Source count, freshness cadence, schema drift risks, missingness patterns, privacy flags.</li>
                    <li><strong>Label strategy:</strong> Manual labeling ops, weak supervision, proxy labels, positive/negative sampling bias.</li>
                    <li><strong>Metric map:</strong> Classification → AUC/PR for imbalance, Precision@K for ranking; Regression → MAE vs. RMSE trade-off.</li>
                    <li><strong>Constraint scan:</strong> Latency budget, memory footprint, explainability needs (LOA compliance), cost per prediction.</li>
                </ul>
            </section>

            <section class="card">
                <h2>Model Families & When They Shine</h2>
                <div class="split">
                    <div>
                        <h3>Linear & Generalized Linear Models</h3>
                        <ul>
                            <li>Baseline for tabular data, fast to train, naturally interpretable via coefficients.</li>
                            <li>Watch for multicollinearity; apply regularization (L1/L2/ElasticNet).</li>
                            <li>Feature scaling critical for gradient convergence.</li>
                        </ul>
                    </div>
                    <div>
                        <h3>Tree Ensembles</h3>
                        <ul>
                            <li><strong>Random Forest:</strong> Low variance via bagging, resilient to noisy features.</li>
                            <li><strong>Gradient Boosting:</strong> GBM/XGBoost/LightGBM/CatBoost for structured data with complex interactions.</li>
                            <li>Interview hook: discuss monotonic constraints, early stopping, feature importance governance.</li>
                        </ul>
                    </div>
                </div>
                <h3>Support Vector Machines</h3>
                <ul>
                    <li>Great for medium-sized, high-dimensional datasets. Kernel trick for non-linear boundaries.</li>
                    <li>Key knobs: kernel choice, C (margin vs. slack), gamma for RBF.</li>
                </ul>
                <h3>Probabilistic & Instance-Based</h3>
                <ul>
                    <li><strong>Naive Bayes:</strong> Text classification baseline, robust under small samples.</li>
                    <li><strong>k-NN:</strong> Lazy learner; emphasize indexing (KD-tree, ball tree) and curse-of-dimensionality mitigations.</li>
                </ul>
                <h3>Unsupervised Primer</h3>
                <ul>
                    <li><strong>Clustering:</strong> K-Means (spherical, balanced), DBSCAN (arbitrary shapes, noise handling), Hierarchical (dendrogram insights).</li>
                    <li><strong>Dimensionality Reduction:</strong> PCA vs. t-SNE/UMAP; talk about variance retention vs. manifold preservation.</li>
                </ul>
            </section>

            <section class="card">
                <h2>Pipeline Architecture Talking Points</h2>
                <div class="diagram">
                    Raw Sources → Data Lake → Validation (Great Expectations) → Feature Store → Training Jobs (scheduled / event-driven) → Model Registry → CI/CD → Serving (batch & online) → Monitoring (drift + business KPIs)
                </div>
                <h3>Feature Engineering</h3>
                <ul>
                    <li>Categoricals: target encoding with leakage guards, embeddings, frequency bucketing.</li>
                    <li>Numericals: scaling, power transforms, winsorization for outliers.</li>
                    <li>Temporal: sliding windows, recency features, time-zone normalization.</li>
                </ul>
                <h3>Experiment Tracking</h3>
                <ul>
                    <li>Tools: MLflow, Weights & Biases, Feast/TFX metadata.</li>
                    <li>Discuss lineage: dataset version → feature view → model artifact → deployment tag.</li>
                </ul>
            </section>

            <section class="card">
                <h2>Tuning & Evaluation Strategy</h2>
                <h3>Hyperparameter Tuning</h3>
                <ul>
                    <li>Grid vs. random search vs. Bayesian optimization; mention early stopping & median pruning.</li>
                    <li>Distributed tuning using Ray Tune/SageMaker; handle resource quotas.</li>
                </ul>
                <h3>Validation Design</h3>
                <ul>
                    <li>K-fold for IID data, time-series split for temporal drift, stratified sampling for class imbalance.</li>
                    <li>Leakage mitigation: fold-aligned preprocessing, purged periods, nested CV for unbiased performance.</li>
                </ul>
                <h3>Production Monitoring</h3>
                <ul>
                    <li>Input drift (Kolmogorov–Smirnov), label drift via delayed ground truth.</li>
                    <li>Business guardrails: conversion, retention, revenue deltas; automated rollback triggers.</li>
                </ul>
            </section>

            <section class="card">
                <h2>Interview Q&A Clusters</h2>
                <div class="qa">
                    <strong>How do you design a fraud detection system?</strong>
                    <ul>
                        <li>Streaming ingestion (Kafka) + feature store with real-time aggregations.</li>
                        <li>Model stack: gradient boosting for primary score, rules engine for hard constraints.</li>
                        <li>Latency budget & fallback path (rule-based) if model unavailable.</li>
                        <li>Feedback loop via analyst labels; active learning for rare events.</li>
                    </ul>
                </div>
                <div class="qa">
                    <strong>Explain feature store value-add.</strong>
                    <ul>
                        <li>Consistent offline/online features, reduces training-serving skew.</li>
                        <li>Access control, documentation, automated freshness monitoring.</li>
                        <li>Supports reuse across teams → accelerates experimentation.</li>
                    </ul>
                </div>
                <div class="qa">
                    <strong>How do you keep models fair and compliant?</strong>
                    <ul>
                        <li>Bias metrics (DP ratio, EO gap) by sensitive cohort.</li>
                        <li>Counterfactual fairness tests: swap sensitive attributes and inspect predictions.</li>
                        <li>Human-in-the-loop reviews for flagged cases, audit trails for regulators.</li>
                    </ul>
                </div>
            </section>

            <section class="card">
                <h2>Deployment Patterns</h2>
                <ul>
                    <li><strong>Batch scoring:</strong> Nightly jobs writing to warehouse (BigQuery/Snowflake) for analytics use cases.</li>
                    <li><strong>Online prediction service:</strong> Containerized model behind API gateway; highlight auto-scaling, canary deploys.</li>
                    <li><strong>Edge deployment:</strong> Model compression (quantization, pruning), update schedule, telemetry backhaul.</li>
                    <li><strong>Champion/Challenger:</strong> Shadow traffic to new model, evaluate before flipping.</li>
                </ul>
                <h3>Safeguards</h3>
                <ul>
                    <li>Feature value range checks, schema signatures, rollback on anomaly.</li>
                    <li>Blue/green switch with config flag; maintain last N models for quick revert.</li>
                </ul>
            </section>
        </div>

        <a class="nav-link" href="../index.html">← Back to ML Playbooks</a>
    </div>
</body>
</html>
